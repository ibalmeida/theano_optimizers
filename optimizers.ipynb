{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(cost, params, learning_rate=0.05):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for param, grad in zip(params, grads):\n",
    "        updates.append((param, param - learning_rate * grad))\n",
    "    return updates\n",
    "\n",
    "def rmsprop(cost, params, learning_rate=1e-3, rho=0.9, epsilon=1e-6):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for param, grad in zip(params, grads):\n",
    "        grad_avg = theano.shared(param.get_value() * 0.)\n",
    "        grad_avg_new = rho * grad_avg + (1 - rho) * grad**2\n",
    "\n",
    "        grad_scaling = T.sqrt(grad_avg_new + epsilon)\n",
    "        grad = grad / grad_scaling\n",
    "\n",
    "        updates.append((grad_avg, grad_avg_new))\n",
    "        updates.append((param, param - learning_rate * grad))\n",
    "    return updates\n",
    "\n",
    "def adagrad(cost, params, learning_rate=1e-3, epsilon=1e-8):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for param, grad in zip(params, grads):\n",
    "        grad_square = theano.shared(param.get_value() * 0.)\n",
    "        grad_square_new = grad_square + grad**2\n",
    "\n",
    "        updates.append((grad_square, grad_square_new))\n",
    "        updates.append((\n",
    "                param,\n",
    "                param - learning_rate * grad / T.sqrt(grad_square_new + epsilon)\n",
    "        ))\n",
    "    return updates\n",
    "\n",
    "def adam2(cost, params, learning_rate=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-3):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    \n",
    "    t = theano.shared(0)\n",
    "    t_new = t + 1\n",
    "    updates.append((t, t_new))\n",
    "    \n",
    "    for param, grad in zip(params, grads):\n",
    "        first_moment = theano.shared(param.get_value() * 0.)\n",
    "        first_moment_new = beta1 * first_moment + (1 - beta1) * grad\n",
    "        first_moment_new = first_moment_new / (1 - beta1**t_new)\n",
    "\n",
    "        second_moment = theano.shared(param.get_value() * 0.)\n",
    "        second_moment_new = beta2 * second_moment + (1 - beta2) * grad**2\n",
    "        second_moment_new = second_moment_new / (1 - beta2**t_new)\n",
    "\n",
    "        updates.append((first_moment, first_moment_new))\n",
    "        updates.append((second_moment, second_moment_new))\n",
    "        updates.append((\n",
    "            param,\n",
    "            param - learning_rate * first_moment_new / T.sqrt(second_moment_new + epsilon)\n",
    "        ))\n",
    "    return updates\n",
    "\n",
    "def adam(cost, params, learning_rate=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    \n",
    "    t = theano.shared(0)\n",
    "    t_new = t + 1\n",
    "    updates.append((t, t_new))\n",
    "    \n",
    "    for param, grad in zip(params, grads):\n",
    "        first_moment = theano.shared(param.get_value() * 0.)\n",
    "        first_moment_new = beta1 * first_moment + (1 - beta1) * grad\n",
    "\n",
    "        second_moment = theano.shared(param.get_value() * 0.)\n",
    "        second_moment_new = beta2 * second_moment + (1 - beta2) * grad**2\n",
    "\n",
    "        learning_rate_norm = learning_rate * T.sqrt(1 - beta2**t_new) / (1 - beta1**t_new)\n",
    "\n",
    "        updates.append((first_moment, first_moment_new))\n",
    "        updates.append((second_moment, second_moment_new))\n",
    "        updates.append((\n",
    "            param,\n",
    "            param - learning_rate_norm * first_moment_new / T.sqrt(second_moment_new + epsilon)\n",
    "        ))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INITIALIZATION\n",
    "# naive guess of the minimum location\n",
    "x = theano.shared(0.5, name='x')\n",
    "y = theano.shared(-3., name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# COST FUNCTION\n",
    "# pick quadratic for simple example/debugging,\n",
    "# Rosen's function for more interesting convergence analysis\n",
    "\n",
    "# global minimum = (0, 0)\n",
    "quad = x**2 + y**2\n",
    "\n",
    "# global minimum = (1, 1)\n",
    "rosen = (1 - x)**2 + 100. * (y - x**2)**2\n",
    "\n",
    "# pick the cost function.\n",
    "cost = rosen\n",
    "f_cost = theano.function([], cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "# pick the optimization method and some meta parameters\n",
    "\n",
    "eta = 0.05\n",
    "#updates = sgd(cost, [x, y], learning_rate=eta)\n",
    "#updates = rmsprop(cost, [x, y], learning_rate=eta)\n",
    "#updates = adagrad(cost, [x, y], learning_rate=eta)\n",
    "updates = adam(cost, [x, y], learning_rate=eta)\n",
    "\n",
    "train = theano.function(\n",
    "    inputs=[],\n",
    "    outputs=cost,\n",
    "    updates=updates \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RUN OPTIMIZATION\n",
    "\n",
    "STR_XY = \"iter: {0:2d}, cost = {1:.2f}, (x, y) = ({2:5.2f}, {3:5.2f})\"\n",
    "costs = [float(f_cost())]\n",
    "xs = [float(x.get_value())]\n",
    "ys = [float(y.get_value())]\n",
    "\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    train()\n",
    "    costs.append(float(f_cost()))\n",
    "    xs.append(float(x.get_value()))\n",
    "    ys.append(float(y.get_value()))\n",
    "    if epoch % (num_epochs / 10) == 0:\n",
    "        print STR_XY.format(epoch, costs[-1], xs[-1], ys[-1])\n",
    "print STR_XY.format(epoch, costs[-1], xs[-1], ys[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PREPARING COST FUNCTION'S LEVEL CURVES\n",
    "x_grid = np.arange(-5.0, 5.0, 0.05)\n",
    "y_grid = np.arange(-5.0, 5.0, 0.05)\n",
    "X, Y = np.meshgrid(x_grid, y_grid)\n",
    "#Z = X**2 + Y**2\n",
    "Z = (1 - X)**2 + 100. * (Y - X**2)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PLOT\n",
    "# plot the evolution of the minimum estimate\n",
    "# together with the functions level curves\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.axes(xlim=(-1.5, 1.5), ylim=(-4, 4))\n",
    "\n",
    "plt.contour(X, Y, Z,\n",
    "           colors='lightgray', levels=np.logspace(-1, 4, 8))\n",
    "plt.plot(xs, ys, '-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ANIMATE\n",
    "# do a looping pop-up animation of the selected algorithm\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(xlim=(-1.5, 1.5), ylim=(-4, 4))\n",
    "line, = ax.plot([], [], '-')\n",
    "\n",
    "def init():\n",
    "    line.set_data([], [])\n",
    "    return line,\n",
    "\n",
    "# animation function. This is called sequentially\n",
    "def animate(i):\n",
    "    line.set_data(xs[:i], ys[:i])\n",
    "    return line,\n",
    "\n",
    "ax.contour(X, Y, Z,\n",
    "           colors='lightgray', levels=np.logspace(-1, 4, 8))\n",
    "ax.plot()\n",
    "\n",
    "# frames is the number of steps + 1 (initial position)\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=len(xs), interval=50, blit=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
